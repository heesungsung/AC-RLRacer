<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Racing prodigy for Assetto Corsa (AC), an agent which learned using our novel RLfD algorithm.">
    <meta property="og:title" content="Efficient Reinforcement Learning for Autonomous Car Racing with Imperfect Demonstrations"/>
    <meta
      property="og:description"
      content=""
    />
    <meta property="og:url" content="https://heesungsung.github.io/AC-RLRacer/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/banner_img.png" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>

    
    <meta name="twitter:title" content="AC-RLRacer">
    <meta name="twitter:description" content="Racing prodigy for Assetto Corsa (AC), an agent which learned using our novel RLfD algorithm.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/banner_img.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Reinforcement Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">



    <title>Efficient Reinforcement Learning for Autonomous Car Racing with Imperfect Demonstrations</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
    <script>
        var players = {};
        const videoIds = [
          { id: 'player1', videoId: '9g7bapVK-MU' }, // SAC(λ)
          { id: 'player2', videoId: '4qI4NcYF-Wg' }, // Demonstration
          { id: 'player3', videoId: 'fsfOFlW_4SA' }, // SACfD
          { id: 'player4', videoId: 'Pn4uDn1wbNg' }, // SACBC
          { id: 'player5', videoId: 'z0zcYjQU8VI' }, // SAC-d
          { id: 'player6', videoId: 'FkphPH0WKuA' }, // SACfD-d(L)
          { id: 'player7', videoId: 'xMMTiS9pzQA' }  // SACfD-d(H)
        ];
        const videoDuration = 113900;

        function onYouTubeIframeAPIReady() {
          videoIds.forEach(({ id, videoId }) => {
            players[id] = new YT.Player(id, {
              height: '100%',
              width: '100%',
              videoId: videoId,
              playerVars: {
                'autoplay': 1,
                'mute': 1,
                'loop': 1,
                'playlist': videoId,
                'modestbranding': 1,
                'showinfo': 0,
                'rel': 0
              },
              events: {
                'onReady': onPlayerReady
              }
            });
          });
        }

        function onPlayerReady(event) {
            if (Object.keys(players).length === videoIds.length) {
                startVideoCycle();
            }
        }

        function startVideoCycle() {
            Object.values(players).forEach(player => {
                player.seekTo(0);
                player.playVideo();
            });

            setTimeout(() => {
                Object.values(players).forEach(player => player.pauseVideo());
            }, videoDuration);

            setTimeout(startVideoCycle, videoDuration);
        }
    </script>
    <script src="https://www.youtube.com/iframe_api"></script>
  </head>


  <body>
    <section class="hero is-hs">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-2 publication-title" style="font-size: 2.2em;">SAC(λ): Efficient Reinforcement Learning for Sparse-Reward Autonomous Car Racing using Imperfect Demonstrations</h1>
              <div class="is-size-4 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  Heeseong Lee,</span>
                <span class="author-block">
                  Sungpyo Sagong,</span>
                <span class="author-block">
                  Minhyeong Lee,</span>
                <span class="author-block">
                  Jeongmin Lee,</span>
                <span class="author-block">
                  Dongjun Lee<sup>†</sup></span>
                  
              </div>

              <div class="is-size-4 publication-authors">
                <span class="author-block">
                  <a href="https://www.inrol.snu.ac.kr/" target="_blank">Seoul National University</a>
                </span><br>
                <span class="corr-author" style="font-size: 0.8em;">
                  <small>† indicates corresponding author</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/INRoL-AD/ACRacer" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser Video-->
    <div class="container is-max-desktop" style="margin: 20px auto;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="full-width-video">
            <iframe src="https://www.youtube-nocookie.com/embed/1xBiWlkgPlc?si=eA5Fzvl_tkght5XG&loop=1&playlist=1xBiWlkgPlc&autoplay=1&mute=1&modestbranding=1&showinfo=0&loop=1&playlist=muJ4fUHyKzo&rel=0"
                    title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin"
                    allowfullscreen>
            </iframe>
          </div>
        </div>
      </div>
    </div>

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Recent advances in Reinforcement Learning (RL) have demonstrated promising results in autonomous car racing. However, two fundamental challenges remain: sparse rewards, which hinder efficient learning process, and the quality of demonstrations, which directly affects the effectiveness of RL from Demonstration (RLfD) approaches. To address these issues, we propose SAC(λ), a novel RLfD algorithm tailored for sparse-reward racing tasks with imperfect demonstrations. SAC(λ) introduces two key components: (1) a discriminator-augmented Q-function, which integrates prior knowledge from demonstrations into value estimation while maintaining off-policy learning benefits, and (2) a Positive-Unlabeled (PU) learning framework with adaptive prior adjustment, which enables the agent to progressively refine its understanding of positive behaviors, while mitigating the overfitting problem. Through extensive experiments in the Assetto Corsa simulator, we demonstrate that SAC(λ) significantly accelerates training, surpasses the provided demonstrations, and achieves superior lap times over existing RL and RLfD approaches
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Paper methodology -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Approach</h2>
            <figure class="image" style="margin-bottom: 1.5rem;">
              <img src="static/images/algo_diagram.png" alt="Algorithm Overview">
            </figure>
            <div class="content has-text-justified">
              <p>
                The above figure shows an overview of SAC(λ). Based on the original SAC algorithm <span style="color: rgba(60, 180, 60, 1.00);">(green shaded box)</span>, additional value from the discriminator is augmented to the original Q-function <span style="color: rgba(255, 0, 0, 0.644);">(red dotted box)</span>. The objective of discriminator is formulated as a positive-unlabeled learning problem, and is learned using given low-quality and narrow-distributed demonstrations.
              </p>
            </div>
            <!-- <figure class="image" style="margin-bottom: 1.5rem; width: 50%; margin: 0 auto;">
              <img src="static/images/pseudocode_simple.png" alt="Pseudo Code">
            </figure> -->
          </div>
        </div>
      </div>
    </section>
    <!-- End paper methodology -->


    <!-- Paper experiment -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Experimental setup</h2>
            <figure class="image" style="margin: 0 auto 1rem auto; width: 80%; text-align: center;">
              <img src="static/images/car_and_track.png" alt="Exp Setup">
            </figure>
            <div class="content has-text-justified">
              <p>
                We consider the lap completion task in car racing, which aims to achieve minimum lap-time.
                Ferrari 458 GT2 is used as our racing car and evaluate on two tracks: Silverstone1967 (S67) and Monza (MNZ). The <span style="color: rgba(255, 0, 0, 0.644);">red dotted boxes</span>
                indicate sections requiring advanced driving strategies
              </p>
              <figure class="image" style="margin: 0 auto 1.5rem auto; width: 100%; text-align: center;">
                <img src="static/images/interface_diagram.png" alt="Interface">
              </figure>
              <p>
                Interfacing framework is built meticulously using AC APIs.
                Overall flow is shown in the figure above.
                Firstly, necessary data such as vehicle velocity, acceleration,
                and contact flags are collected using supported APIs.
                A real-time data parser then passes those data into
                our virtual RL environment, along with additional track data.
                The route manager parses local map data such as preview
                curvatures, slopes, and bank angles, and a 2D rangefinder is implemented.
                The agent interacts with this virtual environment and generates
                actions, which are then passed through
                the virtual gamepad to transfer the control inputs to AC.
                We also provide a random initial spawn function.
              </p>
              <p>
                We set the task which posseses following key challenges :
                <ul>
                  <li>
                    <strong>Sparse reward</strong> : Tasks which require long and precise sequence of actions to successfully finish, designed as a sparse reward problems.
                  </li>
                  <li>
                    <strong>Slow sample collection speed</strong> : Unable to fast-forward or duplicate the environment on a single machine.
                  </li>
                  <li>
                    <strong>Imperfect demonstrations</strong> : Given demonstrations are sub-optimal and narrowly distributed.
                  </li>
                </ul>
                Our goal is to show that SAC(λ) can effectively utilize the given imperfect demonstrations to boost the learning process and achieve better performance than existing methods in the long-horizon and sparse reward tasks, specifically autonomous racing.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper experiment -->


    <!-- Paper results -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Result</h2>
            <div class="content has-text-justified">
              <p>
                We present the experimental results to address the following questions:
                <ul>
                  <li>
                    <strong>Training efficiency</strong> : How effectively does SAC(λ) accelerate the early stages of learning? 
                  </li>
                  <li>
                    <strong>Final Performance</strong> : Does SAC(λ) achieve superior performance after sufficient training steps?
                  </li>
                  <li>
                    <strong>Learned Behavior</strong> : Is the behavior learned by the agent qualitatively comparable to that of a human expert driver?
                  </li>
                  <li>
                    <strong>Effect of λ</strong> : How does the choice of λ affect the entire learning process?
                  </li>
                </ul>
              </p>

              <!-- Question 1 -->
              <h4 class="title is-5"><br>Training Efficiency</h4>
              <figure class="image" style="margin: 0 auto 1rem auto; width: 70%; text-align: center;">
                <img src="static/images/training_efficiency.png" alt="Training Efficiency">
              </figure>
              <p>
                The above figure shows the episode returns
                during the initial stages of training. Training efficiency is
                evaluated based on two criteria: (1) the number of training
                steps required for the agent to complete its first lap and (2)
                the lap time at that episode. Vertical dotted lines in the figure
                represent the average number of steps required for the first
                lap completion. Among all algorithms, SAC(λ) demonstrates the fastest
                learning speed, completing the first lap in fewer steps and
                achieving the highest returns, indicating faster lap times.
                Notably, IL-based algorithms struggle to deal with OOD data as it
                does not leverage online interaction information.
                TRPOfD also fails to complete a lap, as it becomes trapped in a local minimum, as shown below.
              </p>
              <figure class="image" style="margin: 0 auto 1rem auto; width: 70%; text-align: center;">
                <img src="static/videos/OnPolicy/trpofd_square.png" alt="TRPO collapse">
              </figure>
              <div style="display: flex;">
                <div>
                  <div style="border: 5px solid rgb(0, 176, 80); display: inline-block; line-height: 0;">
                    <video id="TRPO_best" autoplay muted playsinline preload="auto" width="100%">
                      <source src="static/videos/OnPolicy/Best.mp4" type="video/mp4">
                    </video>
                  </div>
                  <h6 style="text-align: center;">TRPO Best model before collapse</h6>
                </div>
                <div>
                  <div style="border: 5px solid rgb(255, 0, 0); display: inline-block; line-height: 0;">
                    <video id="TRPO_local_minima" autoplay muted playsinline preload="auto" width="100%">
                      <source src="static/videos/OnPolicy/Local Minima.mp4" type="video/mp4">
                    </video>
                  </div>
                  <h6 style="text-align: center;">TRPO Local minima after collapse</h6>
                </div>
              </div>
              <script>
                document.addEventListener('DOMContentLoaded', () => {
                  const efficiency_video_ids = ['TRPO_best', 'TRPO_local_minima'];

                  function efficiency_video_cycle() {
                    efficiency_video_ids.forEach((id) => {
                      const video = document.getElementById(id);
                      video.currentTime = 0;
                      video.play();
                    });
                    setTimeout(() => {
                      efficiency_video_ids.forEach((id) => {
                        const video = document.getElementById(id);
                        video.pause();
                      });
                    }, 38000);
                    setTimeout(efficiency_video_cycle, 38000);
                  }
                  efficiency_video_cycle();
                });
              </script>


              <!-- Question 2 -->        
              <h4 class="title is-5" style="margin-top: 20px;">
                Final Performance<span style="font-weight: normal;"> : lap time (s)</span>
              </h4>
              <figure class="image" style="margin: 0 auto 1rem auto; width: 100%; text-align: center;">
                <img src="static/images/evaluation_result.png" alt="Final Performance">
              </figure>              
              <div style="margin-bottom: 20px;">
                <div class="video-container" style="width:70%;">
                  <div id="player1"></div><h6 style="text-align: center;">SAC(λ)</h6>
                </div>
              </div>
          
              <div class="video-grid">
                <div class="video-container"><div id="player2"></div><h6 style="text-align: center;">Demonstration</h6></div>
                <div class="video-container"><div id="player3"></div><h6 style="text-align: center;">SACfD</h6></div>
                <div class="video-container"><div id="player4"></div><h6 style="text-align: center;">SACBC</h6></div>
              </div>
          
              <div class="video-grid">
                <div class="video-container"><div id="player5"></div><h6 style="text-align: center;">SAC-d</h6></div>
                <div class="video-container"><div id="player6"></div><h6 style="text-align: center;">SACfD-d(L)</h6></div>
                <div class="video-container"><div id="player7"></div><h6 style="text-align: center;">SACfD-d(H)</h6></div>
              </div>
              
              <p>
                Above table shows the comparison of the best lap times
                achieved after sufficient training steps. The lap time
                for demonstrations indicates the average lap time among the
                existing data. Our approach achieves the fastest lap time
                of 1:29.767 and maintains higher speeds in all sections
                compared to the best performance of the demonstrations and
                other baseline methods.
              </p>

              <!-- Question 3 -->
              <h4 class="title is-5"><br>Learned Behavior</h4>
              <figure class="image" style="margin: 0 0 1rem 0; width: 100%; text-align: center;">
                <img src="static/images/speed_profile.png" alt="Learned Behavior">
              </figure>
              <div style="display: flex;">
                <div>
                  <video id="straight" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Straight.mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Straight</h6>
                </div>
                <div>
                  <video id="chicane" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Chicane (Rl).mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Chicane</h6>
                </div>
                <div>
                  <video id="sweeper" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Sweeper.mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Sweeper</h6>
                </div>
              </div>
              <div style="display: flex;">
                <div>
                  <video id="corners" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Corners.mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Corners</h6>
                </div>
                <div>
                  <video id="kink" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Kink.mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Kink</h6>
                </div>
                <div>
                  <video id="esses" autoplay muted loop playsinline preload="auto" width="100%">
                    <source src="static/videos/Behavior/Esses.mp4" type="video/mp4">
                  </video>
                  <h6 style="text-align: center;">Esses</h6>
                </div>
              </div>
              <p>
                The above figure shows the agent’s trajectory
                for achieving the minimum lap time, with challenging sections
                of varying curvatures selected for detailed visualization. The
                agent effectively uses the full width of the track to minimize
                the curvature of its path and maximize speed, following
                an "out-in-out" trajectory—a technique commonly used by
                expert human drivers. By learning this optimal strategy, the
                agent attains higher speeds through these curve sections,
                resulting in improved overall performance.
              </p>

              <!-- Question 4 -->
              <h4 class="title is-5"><br>Comparison with dense-reward setting, and effect of the λ</h4>
              <figure class="image" style="margin: 0 auto 1rem auto; width: 80%; text-align: center;">
                <img src="static/images/ablation.png" alt="Ablation" style="display: block; margin: auto;">
              </figure>
            
              
              <p>
                The above figure shows evaluation results on the S67 track:
                (a) a comparison of the best lap times against algorithms trained with dense rewards and
                (b) ablation study on the effects of different λ values on SAC(λ) performance.
                Compared to algorithms trained with dense rewards,
                SAC($\lambda$) still demonstrates superior performance
                As λ increases, the algorithm begins
                to leverage offline demonstrations more effectively, reaching
                peak performance at λ = 0.1. Beyond this point, while
                the initial learning speed improves, performance starts to
                decline in the later stages due to the stronger influence of IL.
              </p> 
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper results -->


    <!-- Paper poster -->
    <!-- <section class="section hero is-light">
      <div class="hero-body">
        <div class="container has-text-centered">
          <h2 class="title is-3">Poster</h2>
          <iframe  src="static/pdfs/HSLee_poster.pdf" width="100%" height="1200">
          </iframe>
        </div>
      </div>
    </section> -->
    <!--End paper poster -->


    <!--BibTex citation -->
    <!-- <section class="section" id="BibTeX">
      <div class="container is-max-desktop content has-text-centered">
        <h2 class="title">BibTeX</h2>
        <pre><code>BibTex Code Here</code></pre>
      </div>
    </section> -->
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container has-text-centered">
        <div class="columns is-centered">
          <div class="column is-9">
            <div class="content">
              <p class="white-text">
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                <br>This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>



    <!-- Statcounter tracking code -->
      
    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
</html>
